# 工作记录

## 1.启动openvino环境

首先在Intel 2号机中开启名为 `reid_pose_0` 的虚拟环境

```shell
source activate fast_reid_h
```

然后开启 openvino 的依赖环境

```shell
source /opt/intel/openvino_2021/bin/setupvars.sh
```

之后转到我们的工作空间

```shell
cd /opt/intel/openvino_2021/deployment_tools/open_model_zoo/demos/multi_camera_multi_target_tracking_demo/python
```

## 2.工作内容

因为我们的工作空间是在 `opt` 文件夹下，只有root用户才有权限对其进行修改，因此在Intel 2号机中，我们在 `/home/ubuntu/hws/reid_pose` 文件夹下面创建了我们的一个中转站

在工作空间中，`reid_pose_2.py` 函数是我们的主函数

我们通过运行指令实现我们的最终功能

```shell
python3 reid_pose_4.py -i test_video.mp4 --m_detector /opt/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/intel/pedestrian-detection-adas-0002/FP16/pedestrian-detection-adas-0002.xml --m_reid ~/qmh/Docker_ReID/demo/models/reid/Fast-ReID/mgn_R50-ibn.xml --config configs/person.py --history_file ~/Documents/demo_reid/history_file --save_detections ~/Documents/demo_reid/detections -d CPU --no_show
```

在`reid_pose_2.py` 中，我们找到了 `visualize_multicam_detections` 函数，并对其进行修改

我们要将pose_estimation的工作嵌套入 `/utils/visualizatlsion.py` 中，因此我们对整个工程一下几个文件进行了修改：

1、对 `./reid_pose_4.py` 这个文件：

​	1、各个引用；

​	2、`visualize_multicam_detections` 函数的调用、输入输出相关；

​	3、args部分加入了 cfg 以及另外的 hrnet 的配套配置参数；

​	4、将 3D-CNN直接加入其中。

2、对 `/utils/visualization.py` 文件，我只修改了 39 行的

```python
                width, height, _ = size
```

3、在 `/utils` 文件夹下加入了最重要的 `my_visualization.py` 文件

4、、在 `/utils` 文件夹下加入了 `inference.py` 文件

5、创建了 `/my_model` 文件夹，在这个文件夹下，存放着两个模型的 pth 文件 （骨架模型的 pth 文件为参数文件），同时还有 `pose_hrnet.py` 以及 `default.py` 两个重要的工具文



同时还需要注意的是，因为是远程操作，我把 `./reid_pose_4.py` 中的实时图像输出给去掉了（ 275、276 行）

```python
# if not params.no_show:
# 	cv.imshow(win_name, vis)
```

最后只需要运行：

```shell
python3 reid_pose_4.py -i final_input_1.mp4 --m_detector /opt/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/intel/pedestrian-detection-adas-0002/FP16/pedestrian-detection-adas-0002.xml --m_reid ~/qmh/Docker_ReID/demo/models/reid/Fast-ReID/mgn_R50-ibn.xml --config configs/person.py --history_file ~/Documents/demo_reid/history_file --save_detections ~/Documents/demo_reid/detections -d CPU --output_video /home/ubuntu/hws/reid_pose/output_v.mp4
```



## 3.进一步的

​	在整个工程中，工程核心为 `/utils/my_visualization.py` 文件，其主要内容为：

1、对 **reid** 的输出进行记录，当程序连续检测到同一 **ID** 时，会将其进行记录并对骨架进行重构，进行姿态估计，如果为特定姿态（暂定为down）则会高亮显示该动作。

2、记录下姿态估计的所有输入并对输入进行保存，（保存地址为： `/home/ubuntu/hws/reid_pose/gif_dataset/` ），我们可以将其进行手动标注并以此扩大我们的训练集。



## 4.加入clothes detection

**之前的工作**

​	师兄之前开发的clothes detection的enabler的所处位置为： `/opt/intel/openvino_2021/deployment_tools/open_model_zoo/demos/object_detection_demo/python/demo1.py` 

当然，在同级目录下的 `demo.py` 以及 `fashion_detection.py` 文件也有着相同的功能（只是名字不一样）。

​	经过调研发现，师兄之前开发clothes detection的步骤为：

​	1、在 openvino 框架中寻找可以进行 object detection 的 demo 。

​	2、使用 clothes detection 的数据集对 object detection 中的模型进行训练，保存其 .bin .mapping .xml 文件。

​	3、将新训练的模型载入 object detection demo project ，由此该 project 便可以执行 clothes detection 任务。

 **初步开发工作**

开发思路：

​	本次的开发思路为，寻找 clothes detection 中所用到的模型文件，将其取出并移植到之前的组合 enabler 中。并且，组合 enabler 中的图片处理流程为：reid ——> pose estimation 。所以我们既可以将 clothes detection 放置于 reid 之前也可以放置与 pose estimation 之后，具体情况需要根据组合 enabler 内核的底层逻辑结构决定。

开发难点：

​	1、经过深入了解，两者的底层工程文件的实现方法差距较大，clothes detection 中的 model 直接引自 ie model zoom，封装十分严密，可移植性较低。

​	2、随着 enabler 之间组合的进行，每次 project 的调试成本越来越大，越来越复杂，测试周期不断加长。 

开发步骤：

​	1、在组合 enabler 的工作目录下（ `/opt/intel/openvino_2021/deployment_tools/open_model_zoo/demos/multi_camera_multi_target_tracking_demo/python` ）的 my_model 文件中放入 clothes detection 中的模型文件（需要同时放入 .bin .mapping .xml 文件）。

​	2、在 `reid_pose_6.py` 文件中进行以下修改：

​		· 在 arg 中加入了 `'-clo_m'` ,  `'--clo_model'` 模块，次项中需加入 clothes detection 文件对应的模型文件存储位置（在此工程中，模型文件存储于 my_model 文件夹下）

​		· 新增 `draw_detections(frame, detections, palette, labels, threshold, dataset)` 函数

```python
def draw_detections(frame, detections, palette, labels, threshold, dataset):
    size = frame.shape[:2]
    for detection in detections:
        if detection.score > threshold:
            xmin = max(int(detection.xmin), 0)
            ymin = max(int(detection.ymin), 0)
            xmax = min(int(detection.xmax), size[1])
            ymax = min(int(detection.ymax), size[0])
            class_id = int(detection.id)
            if dataset == 'df2':
                class_name = CLASS_NAMES_DF[class_id]
                color = palette[class_id]
                det_label = labels[class_id] if labels and len(labels) >= class_id else '#{}'.format(class_id)
                cv.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 6)
                cv.putText(frame, '{} {:.1%}'.format(class_name, detection.score),
                            (xmin, ymin - 7), cv.FONT_HERSHEY_COMPLEX, 1, color, 2)
            elif dataset == 'modanet':
                class_name = CLASS_NAMES_MODANET[class_id]
                color = palette[class_id]
                det_label = labels[class_id] if labels and len(labels) >= class_id else '#{}'.format(class_id)
                cv.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 6)
                cv.putText(frame, '{} {:.1%}'.format(class_name, detection.score),
                            (xmin, ymin - 7), cv.FONT_HERSHEY_COMPLEX, 2, color, 3)
            else:
                raise RuntimeError('Invalid dataset name: {}'.format(dataset))
            if isinstance(detection, models.DetectionWithLandmarks):
                for landmark in detection.landmarks:
                    cv.circle(frame, (int(landmark[0]), int(landmark[1])), 2, (0, 255, 255), 2)
    return frame
```

​		· 537 行加入了

```python
    '''
    clothes add
    '''
    log.info("Creating Inference Engine")
    ie = IECore()

    clo_model = models.YOLO(ie, args.clo_model, labels=None, threshold=0.5, keep_aspect_ratio=False)
    detector_pipeline = AsyncPipeline(ie, clo_model, {},
                                      device=args.device, max_num_requests=1)

    palette = ColorPalette(len(clo_model.labels) if clo_model.labels else 100)
    '''
    end
    '''
```

​		· 修改 run 函数中的输入输出

​		· 在 run 函数中 332 行加入（记录参数）

```python
    # clothes add
    next_frame_id = 0
    next_frame_id_to_show = 0
    img_cnt = 0
```

​		· 在 run 函数中 388 行加入

```python
        results = detector_pipeline.get_result(next_frame_id_to_show)
        if results:
            next_frame_id_to_show += 1
            objects, frame_meta = results

            prev_frames[0] = draw_detections(prev_frames[0], objects, palette, clo_model.labels, 0.5, 'df2')


        if detector_pipeline.is_ready():
            detector_pipeline.submit_data(prev_frames[0], next_frame_id, {'frame': prev_frames[0]})
            next_frame_id += 1
```

​		在这里我们需要注意的是，`detector_pipeline` 并不是一个模型，其实一个基于 ie model zoom 的巨型 class，其图像读入方式和组合 enabler 中线程逐帧读入摄像头视频是不一样的，我们不能直接将其图片输出替换线程中的图片流，我们只能将其结果进行记录，然后在另视频流图片中进行标注。

 **运行**

首先在Intel 2号机中开启名为 `reid_pose_0` 的虚拟环境

```shell
source activate fast_reid_h
```

然后开启 openvino 的依赖环境

```shell
source /opt/intel/openvino_2021/bin/setupvars.sh
```

之后转到我们的工作空间

```shell
cd /opt/intel/openvino_2021/deployment_tools/open_model_zoo/demos/multi_camera_multi_target_tracking_demo/python
```

运行下面的指令

```shell
python3 reid_pose_6.py -i new_input_0.mp4 --m_detector /opt/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/intel/pedestrian-detection-adas-0002/FP16/pedestrian-detection-adas-0002.xml --m_reid ~/qmh/Docker_ReID/demo/models/reid/Fast-ReID/mgn_R50-ibn.xml --config configs/person.py --history_file ~/Documents/demo_reid/history_file --save_detections ~/Documents/demo_reid/detections -d CPU --output_video /home/ubuntu/hws/reid_pose/11_22_dir/output_v_3.mp4 --clo_model ./my_model/yolov3-df2.xml 
```



## 5.Pose_estimation 优化工作

在 `/utils/my_visualization.py` 文件中加入了骨架 gif 提取模块，此模块的加入可以让我们在检测 demo 视频时保存所有人体骨架图。

**注：** 当场景中含有多个目标（多人）时，骨架 gif 提取模块依然会将单个骨架提取出来，生成能够用于模型训练的 gif 文件；

​	我们只需要将其下载下来，并进行手动数据集标注（数据集分类）；

​	将新的数据集加入原始数据集中，对原始数据集进行扩充；

​	更新我们的模型文件。



## 6.dress_detection优化工作

**问题：**

​	Dress detection part 的识别准确率过低（总共450帧的视频中，只有不到5帧中的衣物能够被识别）

**step 1**

​	**优化思路：**

​	Dress detection part 在师兄给的demo video的运行效果较好，而在我们的最终project的demo video的运行效果较差，其原因可能是其模型对多目标识的情况下识别准确率较低，也就是其在 one to one 任务中效果较好，但其不能胜任 one to any 的任务。

​	因此 **我们将 dress detection part 的输入从整张图片换成前端截取出的人物图片** 。通过将 one to any 的识别任务转化为 one to one 的任务以提高这部分功能的识别准确率。

​	**优化方法：**

​	在原工作目录下建立文件： `./reid_pose_7.py` 以及 `./utils/my_visualization_2.py` 两个工程文件

​	相较于 `./reid_pose_6.py` ，本次生成的 `./reid_pose_7.py` 主要在两方面进行了修改：

​		1、在引用上，将对 `./utils/my_visualization.py` 的 import 换成了 `./utils/my_visualization_2.py` 。

​		2、取消了 `./reid_pose_6.py` 中的 dress detection part 的线程调用部分，同时 `./reid_pose_7.py` 对 dress detection part 的 init 工作进行了保留，将其模型组传入 `visualize_multicam_detections` 函数中（该函数位于 `./utils/my_visualization_2.py` 内）。

​	相较于 `./utils/my_visualization.py` ，本次生成的 `./utils/my_visualization_2.py` 主要在 `visualize_multicam_detections` 函数以及 `draw_detection_pose` 函数中进行了修改。修改内容为对每帧图像中的人物进行截取，之后再利用 dress detection part 所提供的接口线程进分别对其行处理。

​	**优化效果：**

​	在450帧图像中，一共有10余帧图像中的衣物能够被识别。（但从demo视频的展示效果来看，dress detection part 的效果还是很不好的）

**step 2**

​	**优化思路：**

​	Dress detection part 其内核为 openvino 环境中的 ie model zoom ，给出的接口为模型与线程的结合体，移植的难度较高。师兄开发时直接通过接口对模型进行训练，但训练出的模型泛化能力极低，无法满足现在的需求，就算改为 one to one 的任务也无法对其性能进行有效的提升。同时我也对其内核进行了深挖，详细了解了 openvino 环境的内部结构，因此得以对其提供的模型与线程的结合体进行运用。

​	同时，根据实际需求我们可以发现，对视频中同一 ID 的人物，在一较短的间内（数秒钟），其着装是不会发生改变的，因此我们只需要对每次 dress detection part 的识别结果保留一段时间，如果在这段时间中如果有新的检测结果还能对其内容与保留时间进行更新，由此有效提高 dress detection part 在 demo 视频中的识别效果。

​	**优化方法：**

​	对 `./utils/my_visualization_2.py` 进行大幅修改，同时根据对 `./utils/my_visualization_2.py` 的修改，要对应的对 `./reid_pose_7.py` 中的内容进行对应调整。

​	在 `./utils/my_visualization_2.py` 中，将 **reid enabler** 与 **dress detection enabler** 深度结合，后者依赖前者的多项输出（ **人物位置检测信息**，**人物 ID 信息**， **各类动态数组** ）进行判别以及结果绘制。

​	**优化效果：**

​	在450帧图像中，一共有200余帧图像中的衣物能够被识别，极大提升了 dress detection pair 在 demo video 中的展示效果。

**运行**

首先在Intel 2号机中开启名为 `reid_pose_0` 的虚拟环境

```shell
source activate fast_reid_h
```

然后开启 openvino 的依赖环境

```shell
source /opt/intel/openvino_2021/bin/setupvars.sh
```

之后转到我们的工作空间

```shell
cd /opt/intel/openvino_2021/deployment_tools/open_model_zoo/demos/multi_camera_multi_target_tracking_demo/python
```

运行下面的指令：



```shell
python3 reid_pose_8.py -i new_input_0.mp4 --m_detector /opt/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/intel/pedestrian-detection-adas-0002/FP16/pedestrian-detection-adas-0002.xml --m_reid ~/qmh/Docker_ReID/demo/models/reid/Fast-ReID/mgn_R50-ibn.xml --config configs/person.py --history_file ~/Documents/demo_reid/history_file --save_detections ~/Documents/demo_reid/detections -d CPU --output_video /home/ubuntu/hws/reid_pose/12_1_dir/output_v_5.mp4 --clo_model ./my_model/yolov3-df2.xml --no_show 
```



```shell
python3 reid_pose_8.py -i 'http://pi:raspberry@192.168.12.150:8090/stream.mjpg' --m_detector /opt/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/intel/pedestrian-detection-adas-0002/FP16/pedestrian-detection-adas-0002.xml --m_reid ~/qmh/Docker_ReID/demo/models/reid/Fast-ReID/mgn_R50-ibn.xml --config configs/person.py --history_file ~/Documents/demo_reid/history_file --save_detections ~/Documents/demo_reid/detections -d CPU --output_video /home/ubuntu/hws/reid_pose/12_1_dir/output_v_5.mp4 --clo_model ./my_model/yolov3-df2.xml --no_show 
```



（下面是2021年12月14日根据 leader 提出的新的要求对工程进行的修改）

这里需要将输出便的更加明显……，这里就是加粗加大字体，

创建新的 `reid_pose_10.py` 其目的就是为了引用新的 `./utils/my_visualization_4.py`

在 `./utils/my_visualization_4.py` 中增加每个检测的字体大小。（个人感觉动作识别的大小是够了的）



```shell
python3 reid_pose_10.py -i new_input_0.mp4 --m_detector /opt/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/intel/pedestrian-detection-adas-0002/FP16/pedestrian-detection-adas-0002.xml --m_reid ~/qmh/Docker_ReID/demo/models/reid/Fast-ReID/mgn_R50-ibn.xml --config configs/person.py --history_file ~/Documents/demo_reid/history_file --save_detections ~/Documents/demo_reid/detections -d CPU --output_video /home/ubuntu/hws/reid_pose/12_15_dir/final_output.mp4 --clo_model ./my_model/yolov3-df2.xml --no_show
```



现在是新的最终要求，要求检测特定的动作或者着装，在这里我们就只检测跑和蹲这两个动作。





```shell
python3 reid_pose_newreq.py -i new_input_0.mp4 --m_detector /opt/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/intel/pedestrian-detection-adas-0002/FP16/pedestrian-detection-adas-0002.xml --m_reid ~/qmh/Docker_ReID/demo/models/reid/Fast-ReID/mgn_R50-ibn.xml --config configs/person.py --history_file ~/Documents/demo_reid/history_file --save_detections ~/Documents/demo_reid/detections -d CPU --output_video /home/ubuntu/hws/reid_pose/12_15_dir/final_output_new.mp4 --clo_model ./my_model/yolov3-df2.xml --no_show
```
